{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Misc. Imports\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# STEAD\n",
    "from stead import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations of STEAD Dataset\n",
    "npy_file = '/scratch/cs101/STEAD/stead_full.npy'\n",
    "csv_file = '/scratch/cs101/STEAD/stead_metadata_new.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device settings for PyTorch\n",
    "# If using 'cpu', set 'device_idx' to 0\n",
    "# If using 'cuda', set 'device_idx' to an unused device (check nvidia-smi)\n",
    "device_type = 'cuda'\n",
    "device_idx  = 0\n",
    "device = torch.device(device_type, device_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the impulse signal\n",
    "impulse = SquareImpulse(10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Reading trace files...\n",
      "Loading data into memory...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "full_dataset = STEADDataset(csv_file, npy_file, impulse)\n",
    "# Filter out non-earthquake signals\n",
    "full_dataset.filter(lambda df: df['trace_category'] == 'earthquake_local')\n",
    "# Create train/test split\n",
    "train, test = full_dataset.split(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a mock neural net\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.conv_ds1 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=11, padding=5)\n",
    "        self.conv_ds2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=7, padding=3)\n",
    "        self.conv_ds3 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.conv_ds4 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv_us_p1 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv_us_p2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.conv_us_p3 = nn.Conv1d(in_channels=16, out_channels=8, kernel_size=7, padding=3)\n",
    "        self.conv_us_p4 = nn.Conv1d(in_channels=8, out_channels=1, kernel_size=11, padding=5)\n",
    "        \n",
    "        self.conv_us_s1 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv_us_s2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.conv_us_s3 = nn.Conv1d(in_channels=16, out_channels=8, kernel_size=7, padding=3)\n",
    "        self.conv_us_s4 = nn.Conv1d(in_channels=8, out_channels=1, kernel_size=11, padding=5)\n",
    "        \n",
    "        self.conv_us_c1 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv_us_c2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.conv_us_c3 = nn.Conv1d(in_channels=16, out_channels=8, kernel_size=7, padding=3)\n",
    "        self.conv_us_c4 = nn.Conv1d(in_channels=8, out_channels=1, kernel_size=11, padding=5)\n",
    "                \n",
    "        self.mp1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.us1 = nn.Upsample(scale_factor=2)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Combined encoder\n",
    "        x = self.conv_ds1(x)\n",
    "        x = self.mp1(x)\n",
    "        x = self.conv_ds2(x)\n",
    "        x = self.mp1(x)\n",
    "        x = self.conv_ds3(x)\n",
    "        x = self.mp1(x)\n",
    "        x = self.conv_ds4(x)\n",
    "        x = self.mp1(x)\n",
    "        \n",
    "        # P-arrival Decoder\n",
    "        p = self.us1(x)\n",
    "        p = self.conv_us_p1(p)\n",
    "        p = self.us1(p)\n",
    "        p = self.conv_us_p2(p)\n",
    "        p = self.us1(p)\n",
    "        p = self.conv_us_p3(p)\n",
    "        p = self.us1(p)\n",
    "        p = self.conv_us_p4(p)\n",
    "        \n",
    "        # S-arrival Decoder\n",
    "        s = self.us1(x)\n",
    "        s = self.conv_us_s1(s)\n",
    "        s = self.us1(s)\n",
    "        s = self.conv_us_s2(s)\n",
    "        s = self.us1(s)\n",
    "        s = self.conv_us_s3(s)\n",
    "        s = self.us1(s)\n",
    "        s = self.conv_us_s4(s)\n",
    "        \n",
    "        # Coda Decoder\n",
    "        c = self.us1(x)\n",
    "        c = self.conv_us_c1(c)\n",
    "        c = self.us1(c)\n",
    "        c = self.conv_us_c2(c)\n",
    "        c = self.us1(c)\n",
    "        c = self.conv_us_c3(c)\n",
    "        c = self.us1(c)\n",
    "        c = self.conv_us_c4(c)\n",
    "        \n",
    "        return p, s, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the PyTorch DataLoaders\n",
    "train_data = data.DataLoader(train, batch_size=1000, shuffle=True, num_workers=4)\n",
    "test_data = data.DataLoader(test, batch_size=1000, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard writer\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network, optimization criterion, and optimizer\n",
    "net = Network().to(device)\n",
    "criterion_p = nn.BCEWithLogitsLoss().to(device)\n",
    "criterion_s = nn.BCEWithLogitsLoss().to(device)\n",
    "criterion_c = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/825 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 825/825 [04:13<00:00,  3.26it/s]\n",
      " 66%|██████▌   | 136/207 [00:17<00:08,  8.52it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "# Each iteration is one full pass through the data\n",
    "for e in range(num_epochs):\n",
    "    print(f'Epoch {e}:', end='')\n",
    "    \n",
    "    loss_total = 0\n",
    "    \n",
    "    # Run through the training set\n",
    "    for idx, batch in enumerate(tqdm(train_data)):\n",
    "        # Get inputs and labels, and move them to device\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = [label.to(device) for label in labels]\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make predictions\n",
    "        (p_pred, s_pred, c_pred) = net(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        p_loss = criterion_p(p_pred, labels[0])\n",
    "        s_loss = criterion_s(s_pred, labels[0])\n",
    "        c_loss = criterion_c(c_pred, labels[0])\n",
    "        loss = p_loss + s_loss + c_loss\n",
    "        \n",
    "        # Make optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log loss to TensorBoard\n",
    "        writer.add_scalar('Loss/Batch/Train', loss.item(), e * len(train_data) + idx)\n",
    "        loss_total += loss.item()\n",
    "    \n",
    "    # Log whole-epoch loss average\n",
    "    writer.add_scalar('Loss/Train', loss_total / len(train_data), e)\n",
    "    \n",
    "    loss_total = 0\n",
    "    \n",
    "    # Run through the test set\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data):\n",
    "            # Get inputs and labels, and move them to device\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = [label.to(device) for label in labels]\n",
    "            \n",
    "            # Make predictions\n",
    "            (p_pred, s_pred, c_pred) = net(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            p_loss = criterion_p(p_pred, labels[0])\n",
    "            s_loss = criterion_s(s_pred, labels[0])\n",
    "            c_loss = criterion_c(c_pred, labels[0])\n",
    "            loss = p_loss + s_loss + c_loss\n",
    "            loss_total += loss.item()\n",
    "\n",
    "    # Log loss to TensorBoard\n",
    "    writer.add_scalar('Loss/Test', loss_total / len(test_data), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
